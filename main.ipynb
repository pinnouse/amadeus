{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os, sys\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser, ArgumentTypeError\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from performer_pytorch import PerformerLM\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer, Encoding\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "from adafactor import Adafactor\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('-o', '--o', default='./', dest='output', help='Location of output(s)')\n",
    "parser.add_argument('-c', '--use_cuda', type=str2bool, dest='use_cuda', default=True, help='Use cuda if cuda supported')\n",
    "parser.add_argument('-a', '--artifacts', dest='artifacts', default='./', help='Directory to save artifacts such as checkpoints')\n",
    "parser.add_argument('-e', '--epochs', type=int, dest='train_epochs', default=1, help='Number of epochs to train on')\n",
    "parser.add_argument('-p', '--print_every', type=int, dest='print_every', default=100, help='After how many iterations to print a status')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "use_cuda = args.use_cuda\n",
    "\n",
    "device = 'cuda' if torch.has_cuda and use_cuda else 'cpu'\n",
    "\n",
    "model_dir = args.output\n",
    "artifacts_dir = args.artifacts\n",
    "Path(os.path.join(artifacts_dir, 'checkpoints')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_epochs = max(args.train_epochs, 1)\n",
    "print_every = max(args.print_every, 1)\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('data/bert-base-uncased-vocab.txt', lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing\n",
    "Parse through subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "SEP_TOKEN = '<sep>'\n",
    "\n",
    "multiplier = [60, 60 * 60, 24 * 60 * 60]\n",
    "def get_time(timestr: str) -> int:\n",
    "    time = timestr.split(':')\n",
    "    final_time = 0\n",
    "    ms = float(time[-1]) * 1000\n",
    "    final_time += int(ms)\n",
    "    for i in range(len(time)-2):\n",
    "        t = time[-2-i]\n",
    "        final_time += multiplier[i] * int(t)\n",
    "    return final_time\n",
    "\n",
    "normalize_pattern = re.compile(r'(\\{[\\\\\\*][\\w\\(\\)\\\\\\,\\*]*|\\})', re.M)\n",
    "sub_space = re.compile(r'(\\{|\\\\[nN])', re.M)\n",
    "insert_space = re.compile(r'([\\w\\\"])([\\.\\!\\,\\?])')\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = normalize_pattern.sub('', text)\n",
    "    text = sub_space.sub(' ', text)\n",
    "    text = re.sub(r'([\\'\\\"])', r' \\1 ', text)\n",
    "    text = re.sub(r'([\\.\\!\\?])(\\w)', r'\\1 \\2', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return insert_space.sub(r'\\1 \\2', text)\n",
    "\n",
    "number_match = re.compile(r'\\d+')\n",
    "def match_num(text: str) -> int:\n",
    "    x = number_match.findall(text)\n",
    "    return int(x[0] if len(x) > 0 else 0)\n",
    "\n",
    "class ParsedVocab:\n",
    "    \"\"\"The parsed vocabulary.\"\"\"\n",
    "\n",
    "    def __init__(self, words: List[Tuple[str, int]], longest: int = 0):\n",
    "        words.sort(key=lambda x : x[1], reverse=True)\n",
    "        # words = [('<sos>', 1), ('<eos>', 1)] + words\n",
    "        words = [(PAD_TOKEN, 0), (UNK_TOKEN, 0), (SEP_TOKEN, 0)] + words\n",
    "        self._word2freq = words\n",
    "        self._word2ind = {}\n",
    "        self._words = list(map(lambda x: x[0], words))\n",
    "        self._longest = longest\n",
    "\n",
    "        for i, (w, _) in enumerate(words):\n",
    "            self._word2ind[w] = i\n",
    "\n",
    "    def __getitem__(self, i) -> str:\n",
    "        return self._words[i]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Parsed Vocabulary ({len(self._words)} words)'\n",
    "    \n",
    "    def get_words(self) -> List[str]:\n",
    "        return self._words\n",
    "    \n",
    "    def get_index(self, word) -> int:\n",
    "        if word in self._word2ind:\n",
    "            return self._word2ind[word]\n",
    "        return -1\n",
    "\n",
    "    def sen_to_seq(self, sentence: str, seq_len: int = 0, add_tokens: bool = True, add_pad: bool = False) -> List[int]:\n",
    "        sentence = normalize_text(sentence)\n",
    "        if seq_len <= 0:\n",
    "            seq_len = self._longest\n",
    "        l = []\n",
    "        if add_tokens:\n",
    "            sentence = f'{SOS_TOKEN} {sentence}'\n",
    "        s = sentence.split()\n",
    "        for i in range(min(len(s), seq_len - add_tokens)):\n",
    "            word = s[i]\n",
    "            if word in self._word2ind:\n",
    "                l.append(self._word2ind[word])\n",
    "            else:\n",
    "                l.append(self._word2ind[UNK_TOKEN])\n",
    "        if add_tokens:\n",
    "            l += [self._word2ind[EOS_TOKEN]]\n",
    "        if len(s) < seq_len and add_pad:\n",
    "            l += [self._word2ind[PAD_TOKEN]] * (seq_len - len(l))\n",
    "        return l\n",
    "\n",
    "    def conv_to_seq(self, conversation: List[Dict[str, object]], max_seq_len: int = 0) -> List[int]:\n",
    "        l = [self._word2ind[SOS_TOKEN]]\n",
    "        for conv in conversation:\n",
    "            l.extend(self.sen_to_seq(conv['line'], add_tokens=False))\n",
    "            l.append(self._word2ind[SEP_TOKEN])\n",
    "\n",
    "        if len(l) > 0:\n",
    "            l = l[:max_seq_len]\n",
    "            l[-1] = self._word2ind[EOS_TOKEN]\n",
    "        l += [self._word2ind[PAD_TOKEN]] * (max_seq_len - len(l))\n",
    "        return l\n",
    "        \n",
    "\n",
    "    def gen_mask(self, tokenized_sentence: List[int]) -> List[bool]:\n",
    "        \"\"\"Creates a mask for the given tokenized sentence.\n",
    "\n",
    "        >>> pv = ParsedVocab([('<sos>', 0), ('<eos>', 0), ('hi', 0)])\n",
    "        >>> x = pv.sen_to_seq('<sos> hi <eos> <unk> <unk>', )\n",
    "        >>> pv.gen_mask(x)\n",
    "        [1, 1, 1, 0, 0]\n",
    "        \"\"\"\n",
    "        l = [True] * len(tokenized_sentence)\n",
    "        i = len(tokenized_sentence) - 1\n",
    "        while i >= 0 and tokenized_sentence[i] == self._word2ind[PAD_TOKEN]:\n",
    "            l[i] = False\n",
    "            i -= 1\n",
    "        return l\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Regulard vocabulary for holding the conversations and number of words.\"\"\"\n",
    "\n",
    "    DEFAULT_CONTEXT = 'default'\n",
    "\n",
    "    def __init__(self, conversation_depth: int = 4):\n",
    "        self.words = {}\n",
    "        self._context = Vocab.DEFAULT_CONTEXT\n",
    "        self.conversations = {}\n",
    "        self._held_conversations = {}\n",
    "        self.conversation_depth = conversation_depth\n",
    "        self.longest = 0\n",
    "        self.longest_tokenized = 0\n",
    "\n",
    "    def add_word(self, word: str) -> None:\n",
    "        word = word.lower()\n",
    "        if not word in self.words:\n",
    "            self.words[word] = 0\n",
    "        self.words[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence: str) -> None:\n",
    "        sentence = f'{SOS_TOKEN} {sentence} {EOS_TOKEN}'\n",
    "        [self.add_word(s) for s in sentence.split()]\n",
    "\n",
    "    def switch_context(self, new_context: str) -> None:\n",
    "        if self._context in self._held_conversations and len(self._held_conversations[self._context]) > self.conversation_depth:\n",
    "            self.conversations[self._context].append(self._held_conversations[self._context][\n",
    "                -self.conversation_depth:\n",
    "            ])\n",
    "        self._context = new_context\n",
    "\n",
    "    def add_conversation(self, conversation: Dict[str, object]) -> None:\n",
    "        if not self._context in self.conversations:\n",
    "            self.conversations[self._context] = []\n",
    "            self._held_conversations[self._context] = [conversation]\n",
    "            return\n",
    "        self.add_line(conversation)\n",
    "        lc = self._held_conversations[self._context][-1]\n",
    "        line = lc['line'].split()\n",
    "        if len(line) > self.longest:\n",
    "            self.longest = len(line)\n",
    "        tokenized = tokenizer.encode(lc['line'])\n",
    "        if len(tokenized.ids) > self.longest_tokenized:\n",
    "            self.longest_tokenized = len(tokenized.ids)\n",
    "    \n",
    "    def add_line(self, conversation: Dict[str, object]) -> bool:\n",
    "        if not self._context in self._held_conversations or len(self._held_conversations[self._context]) == 0:\n",
    "            self._held_conversations[self._context] = [conversation]\n",
    "            return True\n",
    "        hc = self._held_conversations[self._context] # Held Conversation\n",
    "        lc = hc[-1] # Last conversation\n",
    "        # Same speaker\n",
    "        if (len(lc['speaker']) > 0 and lc['speaker'] == conversation['speaker']) or \\\n",
    "            (len(lc['speaker']) == 0 and len(conversation['speaker']) == 0 and len(conversation['line']) > 0 and conversation['line'][0].islower()) and \\\n",
    "            conversation['when'] - lc['when'] < 1000 * 60 * 1.5:\n",
    "            hc[-1]['when'] = conversation['when']\n",
    "            hc[-1]['line'] += f\" {conversation['line']}\"\n",
    "            return False\n",
    "        if len(self._held_conversations[self._context]) >= 2:\n",
    "            self.conversations[self._context].append(self._held_conversations[self._context][\n",
    "                -min(self.conversation_depth, len(hc)):\n",
    "            ])\n",
    "        hc.append(conversation)\n",
    "        return True\n",
    "\n",
    "    def parse_vocab(self) -> ParsedVocab:\n",
    "        words = list(self.words.items())\n",
    "        return ParsedVocab(words, self.longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parsing folder: ditfxx_subs\n",
      "  Opening file: DitFXX (1).ass\n",
      "  Opening file: DitFXX (2).ass\n",
      "  Opening file: DitFXX (3).ass\n",
      "  Opening file: DitFXX (4).ass\n",
      "  Opening file: DitFXX (5).ass\n",
      "  Opening file: DitFXX (6).ass\n",
      "  Opening file: DitFXX (7).ass\n",
      "  Opening file: DitFXX (8).ass\n",
      "  Opening file: DitFXX (9).ass\n",
      "  Opening file: DitFXX (10).ass\n",
      "  Opening file: DitFXX (11).ass\n",
      "  Opening file: DitFXX (12).ass\n",
      "  Opening file: DitFXX (13).ass\n",
      "  Opening file: DitFXX (14).ass\n",
      "  Opening file: DitFXX (15).ass\n",
      "  Opening file: DitFXX (16).ass\n",
      "  Opening file: DitFXX (17).ass\n",
      "  Opening file: DitFXX (18).ass\n",
      "  Opening file: DitFXX (19).ass\n",
      "  Opening file: DitFXX (20).ass\n",
      "  Opening file: DitFXX (21).ass\n",
      "  Opening file: DitFXX (22).ass\n",
      "  Opening file: DitFXX (23).ass\n",
      "Parsing folder: steins_gate_subs\n",
      "  Opening file: Steins;Gate 01.ass\n",
      "  Opening file: Steins;Gate 02.ass\n",
      "  Opening file: Steins;Gate 03.ass\n",
      "  Opening file: Steins;Gate 04.ass\n",
      "  Opening file: Steins;Gate 05.ass\n",
      "  Opening file: Steins;Gate 06.ass\n",
      "  Opening file: Steins;Gate 07.ass\n",
      "  Opening file: Steins;Gate 08.ass\n",
      "  Opening file: Steins;Gate 09.ass\n",
      "  Opening file: Steins;Gate 10.ass\n",
      "  Opening file: Steins;Gate 11.ass\n",
      "  Opening file: Steins;Gate 12.ass\n",
      "  Opening file: Steins;Gate 13.ass\n",
      "  Opening file: Steins;Gate 14.ass\n",
      "  Opening file: Steins;Gate 15.ass\n",
      "  Opening file: Steins;Gate 16.ass\n",
      "  Opening file: Steins;Gate 17.ass\n",
      "  Opening file: Steins;Gate 18.ass\n",
      "  Opening file: Steins;Gate 19.ass\n",
      "  Opening file: Steins;Gate 20.ass\n",
      "  Opening file: Steins;Gate 21.ass\n",
      "  Opening file: Steins;Gate 22.ass\n",
      "  Opening file: Steins;Gate 23.ass\n",
      "  Opening file: Steins;Gate 24.ass\n",
      "  Opening file: Steins;Gate 25.ass\n",
      "Done! Num conversations: 12094, num words: 5485, longest convo: 185\n"
     ]
    }
   ],
   "source": [
    "FOLDERS = ['ditfxx_subs', 'steins_gate_subs']\n",
    "CONVERSATION_DEPTH = 4\n",
    "\n",
    "vocab = Vocab(CONVERSATION_DEPTH)\n",
    "\n",
    "for folder in FOLDERS:\n",
    "    dir = os.listdir(os.path.join('data', folder))\n",
    "    dir.sort(key=match_num)\n",
    "    print(f'Parsing folder: {folder}')\n",
    "    for f in dir:\n",
    "        filepath = os.path.join(os.getcwd(), 'data', folder, f)\n",
    "        if not os.path.isfile(filepath): continue\n",
    "        print(f'  Opening file: {f}')\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as sub_file:\n",
    "            is_event = False\n",
    "            line = True\n",
    "            while not is_event and line:\n",
    "                line = sub_file.readline()\n",
    "                if not line: break\n",
    "                if line.rstrip() == \"[Events]\":\n",
    "                    is_event = True\n",
    "            current_format = False\n",
    "            current_conversation = []\n",
    "            \n",
    "            vocab.switch_context(f)\n",
    "            line = True\n",
    "            # for line in sub_file.readlines():\n",
    "            while line:\n",
    "                try:\n",
    "                    line = sub_file.readline()\n",
    "                except UnicodeDecodeError:\n",
    "                    print('    Error decoding a line, skipped.')\n",
    "                if line.startswith('Format:'):\n",
    "                    line = line[len('Format:'):].strip().split(', ')\n",
    "                    current_format = line\n",
    "                    continue\n",
    "                if current_format == False or not line.startswith('Dialogue:'): continue\n",
    "                line = line[len('Dialogue:'):].strip().split(',')\n",
    "                line[len(current_format)-1] = ','.join(line[len(current_format)-1:])\n",
    "                dialogue = dict(zip(current_format, line))\n",
    "                if not dialogue['Style'] in ['main', 'Default']: continue\n",
    "                # Extract variables\n",
    "                speaker = dialogue['Name']\n",
    "                text = normalize_text(dialogue['Text'])\n",
    "                time = get_time(dialogue['Start'])\n",
    "\n",
    "                # if len(current_conversation) > 0 and time - current_conversation[-1]['when'] > 1000 * 60 * 2:\n",
    "                #     current_conversation = []\n",
    "                # if len(current_conversation) > 0 and ((len(speaker) > 0 and current_conversation[-1]['speaker'] == speaker) or \n",
    "                # (len(speaker) == 0 and len(dialogue['Text']) > 0 and dialogue['Text'][0].islower())):\n",
    "                #     current_conversation[-1]['line'] += f' {text}'\n",
    "                #     current_conversation[-1]['when'] = time\n",
    "                # else:\n",
    "                #     vocab.add_conversation(current_conversation)\n",
    "                #     current_conversation.append({\n",
    "                #         'speaker': speaker,\n",
    "                #         'line': text,\n",
    "                #         'when': time\n",
    "                #     })\n",
    "                # if len(current_conversation) > CONVERSATION_DEPTH:\n",
    "                #     current_conversation = current_conversation[CONVERSATION_DEPTH - len(current_conversation):]\n",
    "                # if len(current_conversation) == 1:\n",
    "                #     continue\n",
    "                vocab.add_conversation({\n",
    "                    'speaker': speaker,\n",
    "                    'line': text,\n",
    "                    'when': time\n",
    "                })\n",
    "                vocab.add_sentence(text)\n",
    "            \n",
    "# tokenizer.enable_padding(length=vocab.longest_tokenized)\n",
    "pv = vocab.parse_vocab()\n",
    "convos = 0\n",
    "for k, c in vocab.conversations.items():\n",
    "    convos += len(c)\n",
    "print(f'Done! Num conversations: {convos}, num words: {len(pv.get_words())}, longest convo: {vocab.longest_tokenized}')\n",
    "# print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n['I wanna take a bath .', 'Not again . Show some self-control .']\n['[CLS]', 'i', 'wanna', 'take', 'a', 'bath', '.', '[SEP]', '[CLS]', 'not', 'again', '.', 'show', 'some', 'self', '-', 'control', '.', '[SEP]']\n\n['I wanna take a bath .', 'Not again . Show some self-control .', 'Hey , how do I smell ?']\n['[CLS]', 'i', 'wanna', 'take', 'a', 'bath', '.', '[SEP]', '[CLS]', 'not', 'again', '.', 'show', 'some', 'self', '-', 'control', '.', '[SEP]', '[CLS]', 'hey', ',', 'how', 'do', 'i', 'smell', '?', '[SEP]']\n\n['I wanna take a bath .', 'Not again . Show some self-control .', 'Hey , how do I smell ?', \"Let him rest . He ' s drained after the last battle . Sheesh , what a high-maintenance girl .\"]\n['[CLS]', 'i', 'wanna', 'take', 'a', 'bath', '.', '[SEP]', '[CLS]', 'not', 'again', '.', 'show', 'some', 'self', '-', 'control', '.', '[SEP]', '[CLS]', 'hey', ',', 'how', 'do', 'i', 'smell', '?', '[SEP]', '[CLS]', 'let', 'him', 'rest', '.', 'he', \"'\", 's', 'drained', 'after', 'the', 'last', 'battle', '.', 'she', '##esh', ',', 'what', 'a', 'high', '-', 'maintenance', 'girl', '.', '[SEP]']\n\n['Not again . Show some self-control .', 'Hey , how do I smell ?', \"Let him rest . He ' s drained after the last battle . Sheesh , what a high-maintenance girl .\", 'Does Plantation 13 have an ocean ?']\n['[CLS]', 'not', 'again', '.', 'show', 'some', 'self', '-', 'control', '.', '[SEP]', '[CLS]', 'hey', ',', 'how', 'do', 'i', 'smell', '?', '[SEP]', '[CLS]', 'let', 'him', 'rest', '.', 'he', \"'\", 's', 'drained', 'after', 'the', 'last', 'battle', '.', 'she', '##esh', ',', 'what', 'a', 'high', '-', 'maintenance', 'girl', '.', '[SEP]', '[CLS]', 'does', 'plantation', '13', 'have', 'an', 'ocean', '?', '[SEP]']\n\n['Hey , how do I smell ?', \"Let him rest . He ' s drained after the last battle . Sheesh , what a high-maintenance girl .\", 'Does Plantation 13 have an ocean ?', 'An ocean ?']\n['[CLS]', 'hey', ',', 'how', 'do', 'i', 'smell', '?', '[SEP]', '[CLS]', 'let', 'him', 'rest', '.', 'he', \"'\", 's', 'drained', 'after', 'the', 'last', 'battle', '.', 'she', '##esh', ',', 'what', 'a', 'high', '-', 'maintenance', 'girl', '.', '[SEP]', '[CLS]', 'does', 'plantation', '13', 'have', 'an', 'ocean', '?', '[SEP]', '[CLS]', 'an', 'ocean', '?', '[SEP]']\n30522\n{'length': None, 'pad_to_multiple_of': None, 'pad_id': 0, 'pad_token': '[PAD]', 'pad_type_id': 0, 'direction': 'right'}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'speaker': 'Z2', 'line': 'I wanna take a bath .', 'when': 150},\n",
       " {'speaker': 'Franxx',\n",
       "  'line': 'Not again . Show some self-control .',\n",
       "  'when': 2160}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "x = list(vocab.conversations)\n",
    "c = vocab.conversations[x[0]]\n",
    "\n",
    "for cc in c[:5]:\n",
    "    l = [x['line'] for x in cc]\n",
    "    print(f'\\n{l}')\n",
    "    output = tokenizer.encode_batch(l)\n",
    "    # print([o.tokens for o in output])\n",
    "    # print([o.tokens[0 if i == 0 else 1:] for i, o in enumerate(output)])\n",
    "    # cat = sum([o.ids[0 if i == 0 else 1:] for i, o in enumerate(output)], [])\n",
    "    # output[1].pad(100)\n",
    "    # print(output[0].merge(output[1:2]).ids)\n",
    "    # print(tokenizer.decode(cat))\n",
    "    # output = tokenizer.encode(l[0], pair=l[1])\n",
    "    output = Encoding.merge(output, growing_offsets=False)\n",
    "    print(output.tokens)\n",
    "\n",
    "print(tokenizer.get_vocab_size())\n",
    "\n",
    "tokenizer.enable_padding()\n",
    "print(tokenizer.padding)\n",
    "\n",
    "c[0][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "Defining the actual AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 555, 30522])\ntorch.Size([1, 555])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<graphviz.dot.Digraph at 0x2b49a7a4cd0>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n -->\r\n<!-- Title: %3 Pages: 1 -->\r\n<svg width=\"443pt\" height=\"822pt\"\r\n viewBox=\"0.00 0.00 442.50 822.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 818)\">\r\n<title>%3</title>\r\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-818 438.5,-818 438.5,4 -4,4\"/>\r\n<!-- 2974709075776 -->\r\n<g id=\"node1\" class=\"node\"><title>2974709075776</title>\r\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"386.5,-21 290.5,-21 290.5,-0 386.5,-0 386.5,-21\"/>\r\n<text text-anchor=\"middle\" x=\"338.5\" y=\"-7.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">MeanBackward0</text>\r\n</g>\r\n<!-- 2974709074624 -->\r\n<g id=\"node2\" class=\"node\"><title>2974709074624</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"383.5,-78 293.5,-78 293.5,-57 383.5,-57 383.5,-78\"/>\r\n<text text-anchor=\"middle\" x=\"338.5\" y=\"-64.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddBackward0</text>\r\n</g>\r\n<!-- 2974709074624&#45;&gt;2974709075776 -->\r\n<g id=\"edge1\" class=\"edge\"><title>2974709074624&#45;&gt;2974709075776</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M338.5,-56.9197C338.5,-49.9083 338.5,-40.1442 338.5,-31.4652\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"342,-31.3408 338.5,-21.3408 335,-31.3409 342,-31.3408\"/>\r\n</g>\r\n<!-- 2974709073040 -->\r\n<g id=\"node3\" class=\"node\"><title>2974709073040</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"340,-141.5 221,-141.5 221,-120.5 340,-120.5 340,-141.5\"/>\r\n<text text-anchor=\"middle\" x=\"280.5\" y=\"-127.9\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">UnsafeViewBackward</text>\r\n</g>\r\n<!-- 2974709073040&#45;&gt;2974709074624 -->\r\n<g id=\"edge2\" class=\"edge\"><title>2974709073040&#45;&gt;2974709074624</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M289.563,-120.391C298.279,-111.148 311.654,-96.966 322.22,-85.7628\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"324.995,-87.9213 329.31,-78.2449 319.902,-83.1186 324.995,-87.9213\"/>\r\n</g>\r\n<!-- 2974709073232 -->\r\n<g id=\"node4\" class=\"node\"><title>2974709073232</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"322,-205 239,-205 239,-184 322,-184 322,-205\"/>\r\n<text text-anchor=\"middle\" x=\"280.5\" y=\"-191.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">MmBackward</text>\r\n</g>\r\n<!-- 2974709073232&#45;&gt;2974709073040 -->\r\n<g id=\"edge3\" class=\"edge\"><title>2974709073232&#45;&gt;2974709073040</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M280.5,-183.891C280.5,-175.366 280.5,-162.639 280.5,-151.923\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"284,-151.745 280.5,-141.745 277,-151.745 284,-151.745\"/>\r\n</g>\r\n<!-- 2974709073616 -->\r\n<g id=\"node5\" class=\"node\"><title>2974709073616</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"275.5,-262 187.5,-262 187.5,-241 275.5,-241 275.5,-262\"/>\r\n<text text-anchor=\"middle\" x=\"231.5\" y=\"-248.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">ViewBackward</text>\r\n</g>\r\n<!-- 2974709073616&#45;&gt;2974709073232 -->\r\n<g id=\"edge4\" class=\"edge\"><title>2974709073616&#45;&gt;2974709073232</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M240.035,-240.92C246.906,-233.207 256.745,-222.164 265,-212.898\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"267.694,-215.136 271.733,-205.341 262.467,-210.479 267.694,-215.136\"/>\r\n</g>\r\n<!-- 2974771108784 -->\r\n<g id=\"node6\" class=\"node\"><title>2974771108784</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"296,-325.5 149,-325.5 149,-304.5 296,-304.5 296,-325.5\"/>\r\n<text text-anchor=\"middle\" x=\"222.5\" y=\"-311.9\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">NativeLayerNormBackward</text>\r\n</g>\r\n<!-- 2974771108784&#45;&gt;2974709073616 -->\r\n<g id=\"edge5\" class=\"edge\"><title>2974771108784&#45;&gt;2974709073616</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M223.906,-304.391C225.154,-295.866 227.016,-283.139 228.584,-272.423\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"232.089,-272.646 230.074,-262.245 225.163,-271.633 232.089,-272.646\"/>\r\n</g>\r\n<!-- 2974771105952 -->\r\n<g id=\"node7\" class=\"node\"><title>2974771105952</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"168.5,-395.5 78.5,-395.5 78.5,-374.5 168.5,-374.5 168.5,-395.5\"/>\r\n<text text-anchor=\"middle\" x=\"123.5\" y=\"-381.9\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">SumBackward1</text>\r\n</g>\r\n<!-- 2974771105952&#45;&gt;2974771108784 -->\r\n<g id=\"edge6\" class=\"edge\"><title>2974771105952&#45;&gt;2974771108784</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M137.684,-374.257C154.023,-363.034 181.074,-344.454 200.229,-331.297\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"202.4,-334.052 208.661,-325.505 198.437,-328.282 202.4,-334.052\"/>\r\n</g>\r\n<!-- 2974771106336 -->\r\n<g id=\"node8\" class=\"node\"><title>2974771106336</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"168.5,-459 78.5,-459 78.5,-438 168.5,-438 168.5,-459\"/>\r\n<text text-anchor=\"middle\" x=\"123.5\" y=\"-445.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">StackBackward</text>\r\n</g>\r\n<!-- 2974771106336&#45;&gt;2974771105952 -->\r\n<g id=\"edge7\" class=\"edge\"><title>2974771106336&#45;&gt;2974771105952</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M123.5,-437.891C123.5,-429.366 123.5,-416.639 123.5,-405.923\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"127,-405.745 123.5,-395.745 120,-405.745 127,-405.745\"/>\r\n</g>\r\n<!-- 2974771106192 -->\r\n<g id=\"node9\" class=\"node\"><title>2974771106192</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"165.5,-516 81.5,-516 81.5,-495 165.5,-495 165.5,-516\"/>\r\n<text text-anchor=\"middle\" x=\"123.5\" y=\"-502.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">SplitBackward</text>\r\n</g>\r\n<!-- 2974771106192&#45;&gt;2974771106336 -->\r\n<g id=\"edge8\" class=\"edge\"><title>2974771106192&#45;&gt;2974771106336</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M118.233,-494.92C116.855,-487.908 116.437,-478.144 116.98,-469.465\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"120.479,-469.684 118.183,-459.341 113.528,-468.858 120.479,-469.684\"/>\r\n</g>\r\n<!-- 2974771106192&#45;&gt;2974771106336 -->\r\n<g id=\"edge17\" class=\"edge\"><title>2974771106192&#45;&gt;2974771106336</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M128.767,-494.92C130.145,-487.908 130.563,-478.144 130.02,-469.465\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"133.472,-468.858 128.817,-459.341 126.521,-469.684 133.472,-468.858\"/>\r\n</g>\r\n<!-- 2974781940400 -->\r\n<g id=\"node10\" class=\"node\"><title>2974781940400</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"201.5,-573 45.5,-573 45.5,-552 201.5,-552 201.5,-573\"/>\r\n<text text-anchor=\"middle\" x=\"123.5\" y=\"-559.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">_ReversibleFunctionBackward</text>\r\n</g>\r\n<!-- 2974781940400&#45;&gt;2974771106192 -->\r\n<g id=\"edge9\" class=\"edge\"><title>2974781940400&#45;&gt;2974771106192</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M123.5,-551.92C123.5,-544.908 123.5,-535.144 123.5,-526.465\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"127,-526.341 123.5,-516.341 120,-526.341 127,-526.341\"/>\r\n</g>\r\n<!-- 2974771106528 -->\r\n<g id=\"node11\" class=\"node\"><title>2974771106528</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"163.5,-630 83.5,-630 83.5,-609 163.5,-609 163.5,-630\"/>\r\n<text text-anchor=\"middle\" x=\"123.5\" y=\"-616.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">CatBackward</text>\r\n</g>\r\n<!-- 2974771106528&#45;&gt;2974781940400 -->\r\n<g id=\"edge10\" class=\"edge\"><title>2974771106528&#45;&gt;2974781940400</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M123.5,-608.92C123.5,-601.908 123.5,-592.144 123.5,-583.465\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"127,-583.341 123.5,-573.341 120,-583.341 127,-583.341\"/>\r\n</g>\r\n<!-- 2974771106720 -->\r\n<g id=\"node12\" class=\"node\"><title>2974771106720</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"168.5,-687 78.5,-687 78.5,-666 168.5,-666 168.5,-687\"/>\r\n<text text-anchor=\"middle\" x=\"123.5\" y=\"-673.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddBackward0</text>\r\n</g>\r\n<!-- 2974771106720&#45;&gt;2974771106528 -->\r\n<g id=\"edge11\" class=\"edge\"><title>2974771106720&#45;&gt;2974771106528</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M118.233,-665.92C116.855,-658.908 116.437,-649.144 116.98,-640.465\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"120.479,-640.684 118.183,-630.341 113.528,-639.858 120.479,-640.684\"/>\r\n</g>\r\n<!-- 2974771106720&#45;&gt;2974771106528 -->\r\n<g id=\"edge16\" class=\"edge\"><title>2974771106720&#45;&gt;2974771106528</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M128.767,-665.92C130.145,-658.908 130.563,-649.144 130.02,-640.465\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"133.472,-639.858 128.817,-630.341 126.521,-640.684 133.472,-639.858\"/>\r\n</g>\r\n<!-- 2974771106912 -->\r\n<g id=\"node13\" class=\"node\"><title>2974771106912</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"115,-744 0,-744 0,-723 115,-723 115,-744\"/>\r\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-730.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">EmbeddingBackward</text>\r\n</g>\r\n<!-- 2974771106912&#45;&gt;2974771106720 -->\r\n<g id=\"edge12\" class=\"edge\"><title>2974771106912&#45;&gt;2974771106720</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M68.9963,-722.92C78.6253,-714.896 92.5807,-703.266 103.96,-693.784\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"106.249,-696.431 111.691,-687.341 101.768,-691.054 106.249,-696.431\"/>\r\n</g>\r\n<!-- 2974771106816 -->\r\n<g id=\"node14\" class=\"node\"><title>2974771106816</title>\r\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"107.5,-814 7.5,-814 7.5,-780 107.5,-780 107.5,-814\"/>\r\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-800.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">token_emb.weight</text>\r\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-787.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\"> (30522, 512)</text>\r\n</g>\r\n<!-- 2974771106816&#45;&gt;2974771106912 -->\r\n<g id=\"edge13\" class=\"edge\"><title>2974771106816&#45;&gt;2974771106912</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M57.5,-779.842C57.5,-772.012 57.5,-762.54 57.5,-754.282\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"61.0001,-754.042 57.5,-744.042 54.0001,-754.042 61.0001,-754.042\"/>\r\n</g>\r\n<!-- 2974771105904 -->\r\n<g id=\"node15\" class=\"node\"><title>2974771105904</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"248,-744 133,-744 133,-723 248,-723 248,-744\"/>\r\n<text text-anchor=\"middle\" x=\"190.5\" y=\"-730.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">EmbeddingBackward</text>\r\n</g>\r\n<!-- 2974771105904&#45;&gt;2974771106720 -->\r\n<g id=\"edge14\" class=\"edge\"><title>2974771105904&#45;&gt;2974771106720</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M178.829,-722.92C169.055,-714.896 154.888,-703.266 143.337,-693.784\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"145.438,-690.981 135.488,-687.341 140.996,-696.391 145.438,-690.981\"/>\r\n</g>\r\n<!-- 2974771107008 -->\r\n<g id=\"node16\" class=\"node\"><title>2974771107008</title>\r\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"236,-814 145,-814 145,-780 236,-780 236,-814\"/>\r\n<text text-anchor=\"middle\" x=\"190.5\" y=\"-800.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">pos_emb.weight</text>\r\n<text text-anchor=\"middle\" x=\"190.5\" y=\"-787.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\"> (555, 512)</text>\r\n</g>\r\n<!-- 2974771107008&#45;&gt;2974771105904 -->\r\n<g id=\"edge15\" class=\"edge\"><title>2974771107008&#45;&gt;2974771105904</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M190.5,-779.842C190.5,-772.012 190.5,-762.54 190.5,-754.282\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"194,-754.042 190.5,-744.042 187,-754.042 194,-754.042\"/>\r\n</g>\r\n<!-- 2974771106480 -->\r\n<g id=\"node17\" class=\"node\"><title>2974771106480</title>\r\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"258.5,-402 186.5,-402 186.5,-368 258.5,-368 258.5,-402\"/>\r\n<text text-anchor=\"middle\" x=\"222.5\" y=\"-388.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">norm.weight</text>\r\n<text text-anchor=\"middle\" x=\"222.5\" y=\"-375.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\"> (512)</text>\r\n</g>\r\n<!-- 2974771106480&#45;&gt;2974771108784 -->\r\n<g id=\"edge18\" class=\"edge\"><title>2974771106480&#45;&gt;2974771108784</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M222.5,-367.885C222.5,-358.309 222.5,-346.088 222.5,-335.912\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"226,-335.895 222.5,-325.895 219,-335.895 226,-335.895\"/>\r\n</g>\r\n<!-- 2974771106000 -->\r\n<g id=\"node18\" class=\"node\"><title>2974771106000</title>\r\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"338,-402 277,-402 277,-368 338,-368 338,-402\"/>\r\n<text text-anchor=\"middle\" x=\"307.5\" y=\"-388.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">norm.bias</text>\r\n<text text-anchor=\"middle\" x=\"307.5\" y=\"-375.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\"> (512)</text>\r\n</g>\r\n<!-- 2974771106000&#45;&gt;2974771108784 -->\r\n<g id=\"edge19\" class=\"edge\"><title>2974771106000&#45;&gt;2974771108784</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M287.356,-367.885C273.905,-357.124 256.278,-343.023 242.796,-332.237\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"244.616,-329.21 234.621,-325.696 240.243,-334.676 244.616,-329.21\"/>\r\n</g>\r\n<!-- 2974709073280 -->\r\n<g id=\"node19\" class=\"node\"><title>2974709073280</title>\r\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"383,-262 312,-262 312,-241 383,-241 383,-262\"/>\r\n<text text-anchor=\"middle\" x=\"347.5\" y=\"-248.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">TBackward</text>\r\n</g>\r\n<!-- 2974709073280&#45;&gt;2974709073232 -->\r\n<g id=\"edge20\" class=\"edge\"><title>2974709073280&#45;&gt;2974709073232</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M335.829,-240.92C326.055,-232.896 311.888,-221.266 300.337,-211.784\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"302.438,-208.981 292.488,-205.341 297.996,-214.391 302.438,-208.981\"/>\r\n</g>\r\n<!-- 2974771105856 -->\r\n<g id=\"node20\" class=\"node\"><title>2974771105856</title>\r\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"401,-332 314,-332 314,-298 401,-298 401,-332\"/>\r\n<text text-anchor=\"middle\" x=\"357.5\" y=\"-318.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">to_logits.weight</text>\r\n<text text-anchor=\"middle\" x=\"357.5\" y=\"-305.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\"> (30522, 512)</text>\r\n</g>\r\n<!-- 2974771105856&#45;&gt;2974709073280 -->\r\n<g id=\"edge21\" class=\"edge\"><title>2974771105856&#45;&gt;2974709073280</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M354.873,-297.842C353.585,-289.923 352.024,-280.324 350.671,-272.001\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"354.111,-271.351 349.052,-262.042 347.202,-272.474 354.111,-271.351\"/>\r\n</g>\r\n<!-- 2974709075440 -->\r\n<g id=\"node21\" class=\"node\"><title>2974709075440</title>\r\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"434.5,-148 358.5,-148 358.5,-114 434.5,-114 434.5,-148\"/>\r\n<text text-anchor=\"middle\" x=\"396.5\" y=\"-134.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">to_logits.bias</text>\r\n<text text-anchor=\"middle\" x=\"396.5\" y=\"-121.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\"> (30522)</text>\r\n</g>\r\n<!-- 2974709075440&#45;&gt;2974709074624 -->\r\n<g id=\"edge22\" class=\"edge\"><title>2974709075440&#45;&gt;2974709074624</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M381.261,-113.842C373.038,-105.122 362.894,-94.3655 354.549,-85.5176\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"356.906,-82.9156 347.499,-78.0419 351.814,-87.7183 356.906,-82.9156\"/>\r\n</g>\r\n</g>\r\n</svg>\r\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# 3 Sentences with 2 delims\n",
    "seq_len = vocab.longest_tokenized * (vocab.conversation_depth - 1)\n",
    "\n",
    "# Thanks to\n",
    "# https://github.com/lucidrains/performer-pytorch\n",
    "model = PerformerLM(\n",
    "    num_tokens=tokenizer.get_vocab_size(),\n",
    "    max_seq_len=seq_len,\n",
    "    dim=512,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    causal=False,\n",
    "    nb_features=256,\n",
    "    generalized_attention=True,\n",
    "    kernel_fn=nn.ReLU(),\n",
    "    reversible=True,\n",
    "    ff_chunks=10,\n",
    "    use_scalenorm=False,\n",
    "    use_rezero=True\n",
    ")\n",
    "\n",
    "x = torch.randint(0, len(pv.get_words()), (1, seq_len))\n",
    "mask = torch.ones_like(x).bool()\n",
    "\n",
    "y = model(x, mask=mask)\n",
    "\n",
    "print(y.size())\n",
    "print(x.shape)\n",
    "\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class ConversationIter:\n",
    "\n",
    "    def __init__(self, vocab: Vocab, parsed_vocab: ParsedVocab, max_seq_len: int):\n",
    "        self._vocab = vocab\n",
    "        self._parsed_vocab = parsed_vocab\n",
    "        self._context = random.choice(list(vocab.conversations))\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self._i = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._context = random.choice(list(self._vocab.conversations))\n",
    "        self._i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> Tuple[Encoding, Encoding]:\n",
    "        if self._i >= len(self._vocab.conversations[self._context]):\n",
    "            raise StopIteration\n",
    "        x = self._vocab.conversations[self._context][self._i]\n",
    "        l = [tokenizer.encode(y['line']) for y in x]\n",
    "        # i = Encoding.merge()\n",
    "        input = Encoding.merge(l[:-1])\n",
    "        target = l[-1]\n",
    "        input.pad(seq_len)\n",
    "        target.pad(seq_len)\n",
    "        # i = sum([j.ids[0 if i == 0 else 1:] for i, j in enumerate(l[-1])], [])\n",
    "        # t = l[-1].ids\n",
    "        # i.extend([tokenizer.pad_id] * (len(i) - vocab.longest_tokenized))\n",
    "        # input = torch.tensor()\n",
    "        # target = torch.tensor(l[-1].ids)\n",
    "        # input = torch.tensor(self._parsed_vocab.conv_to_seq(x[:-1], self.max_seq_len))\n",
    "        # target = torch.tensor(self._parsed_vocab.sen_to_seq(x[-1]['line'], seq_len=self.max_seq_len, add_pad=True))\n",
    "        self._i += 1\n",
    "        return input, target\n",
    "\n",
    "def train(conv_iter: ConversationIter):\n",
    "    model.train()\n",
    "    accrued_loss = 0\n",
    "    start = datetime.now()\n",
    "    for i, (input, target) in enumerate(conv_iter):\n",
    "        # mask = torch.tensor(pv.gen_mask(input))\n",
    "        mask = torch.tensor(input.attention_mask).bool()\n",
    "\n",
    "        input = torch.tensor(input.ids)\n",
    "        target = torch.tensor(target.ids)\n",
    "\n",
    "        input.to(device)\n",
    "        target.to(device)\n",
    "\n",
    "        input.unsqueeze_(0)\n",
    "        target.unsqueeze_(0)\n",
    "        mask.unsqueeze_(0)\n",
    "\n",
    "#         input = F.one_hot(input, len(pv.get_words()))\n",
    "#         input.transpose_(0, 1)\n",
    "\n",
    "#         print(input.shape, target.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, mask=mask)\n",
    "#         output.transpose_(1, 2)\n",
    "        loss = criterion(output.squeeze(0), target.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accrued_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % print_every == 0:\n",
    "            print(f'  Iter {i+1} (Took {(datetime.now() - start).total_seconds():.3f}s): AverageLoss: {accrued_loss/print_every:.4f}')\n",
    "            accrued_loss = 0\n",
    "            start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training epoch #1 of 1:\n",
      "  Iter 100 (Took 263.392s): AverageLoss: 1.5456\n",
      "  Iter 200 (Took 263.764s): AverageLoss: 0.1197\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-705039d67580>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training epoch #{epoch+1} of {train_epochs}:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch+1} took {(datetime.now()-total).total_seconds():.3f}s\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c679248050b6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(conv_iter)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m#         output.transpose_(1, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SAVE_EVERY = 5\n",
    "\n",
    "conv_iter = ConversationIter(vocab, pv, seq_len)\n",
    "\n",
    "def save_checkpoint(epoch: int):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, os.path.join(artifacts_dir, f'checkpoints/amadeus-performer-{epoch}.pt'))\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    print(f'Training epoch #{epoch+1} of {train_epochs}:')\n",
    "    total = datetime.now()\n",
    "    train(conv_iter)\n",
    "    print(f'Epoch {epoch+1} took {(datetime.now()-total).total_seconds():.3f}s\\n\\n')\n",
    "    \n",
    "    if (epoch + 1) % SAVE_EVERY == 0:\n",
    "        print('Saving checkpoint...')\n",
    "        save_checkpoint(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}