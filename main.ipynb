{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os, sys\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser, ArgumentTypeError\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from performer_pytorch import PerformerLM\n",
    "\n",
    "# from torchviz import make_dot\n",
    "\n",
    "from adafactor import Adafactor\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "       return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('-o', '--o', default='./', dest='output', help='Location of output(s)')\n",
    "parser.add_argument('-c', '--use_cuda', type=str2bool, dest='use_cuda', default=True, help=\"Use cuda if cuda supported\")\n",
    "\n",
    "use_cuda = parser.parse_args().use_cuda\n",
    "\n",
    "device = 'cuda' if torch.has_cuda and use_cuda else 'cpu'\n",
    "\n",
    "model_dir = parser.parse_args().output\n",
    "Path(os.path.join(model_dir, 'checkpoints')).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing\n",
    "Parse through subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "SEP_TOKEN = '<sep>'\n",
    "\n",
    "multiplier = [60, 60 * 60, 24 * 60 * 60]\n",
    "def get_time(timestr: str) -> int:\n",
    "    time = timestr.split(':')\n",
    "    final_time = 0\n",
    "    ms = float(time[-1]) * 1000\n",
    "    final_time += int(ms)\n",
    "    for i in range(len(time)-2):\n",
    "        t = time[-2-i]\n",
    "        final_time += multiplier[i] * int(t)\n",
    "    return final_time\n",
    "\n",
    "normalize_pattern = re.compile(r'(\\{[\\\\\\*][\\w\\(\\)\\\\\\,\\*]*|\\})', re.M)\n",
    "sub_space = re.compile(r'(\\{|\\\\[nN])', re.M)\n",
    "insert_space = re.compile(r'([\\w\\\"])([\\.\\!\\,\\?])')\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = normalize_pattern.sub('', text)\n",
    "    text = sub_space.sub(' ', text)\n",
    "    text = re.sub(r'([\\'\\\"])', r' \\1 ', text)\n",
    "    text = re.sub(r'([\\.\\!\\?])(\\w)', r'\\1 \\2', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return insert_space.sub(r'\\1 \\2', text)\n",
    "\n",
    "number_match = re.compile(r'\\d+')\n",
    "def match_num(text: str) -> int:\n",
    "    x = number_match.findall(text)\n",
    "    return int(x[0] if len(x) > 0 else 0)\n",
    "\n",
    "class ParsedVocab:\n",
    "    \"\"\"The parsed vocabulary.\"\"\"\n",
    "\n",
    "    def __init__(self, words: List[Tuple[str, int]], longest: int = 0):\n",
    "        words.sort(key=lambda x : x[1], reverse=True)\n",
    "        # words = [('<sos>', 1), ('<eos>', 1)] + words\n",
    "        words = [(PAD_TOKEN, 0), (UNK_TOKEN, 0), (SEP_TOKEN, 0)] + words\n",
    "        self._word2freq = words\n",
    "        self._word2ind = {}\n",
    "        self._words = list(map(lambda x: x[0], words))\n",
    "        self._longest = longest\n",
    "\n",
    "        for i, (w, _) in enumerate(words):\n",
    "            self._word2ind[w] = i\n",
    "\n",
    "    def __getitem__(self, i) -> str:\n",
    "        return self._words[i]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Parsed Vocabulary ({len(self._words)} words)'\n",
    "    \n",
    "    def get_words(self) -> List[str]:\n",
    "        return self._words\n",
    "    \n",
    "    def get_index(self, word) -> int:\n",
    "        if word in self._word2ind:\n",
    "            return self._word2ind[word]\n",
    "        return -1\n",
    "\n",
    "    def sen_to_seq(self, sentence: str, seq_len: int = 0, add_tokens: bool = True, add_pad: bool = False) -> List[int]:\n",
    "        sentence = normalize_text(sentence)\n",
    "        if seq_len <= 0:\n",
    "            seq_len = self._longest\n",
    "        l = []\n",
    "        if add_tokens:\n",
    "            sentence = f'{SOS_TOKEN} {sentence}'\n",
    "        s = sentence.split()\n",
    "        for i in range(min(len(s), seq_len - add_tokens)):\n",
    "            word = s[i]\n",
    "            if word in self._word2ind:\n",
    "                l.append(self._word2ind[word])\n",
    "            else:\n",
    "                l.append(self._word2ind[UNK_TOKEN])\n",
    "        if add_tokens:\n",
    "            l += [self._word2ind[EOS_TOKEN]]\n",
    "        if len(s) < seq_len and add_pad:\n",
    "            l += [self._word2ind[PAD_TOKEN]] * (seq_len - len(l))\n",
    "        return l\n",
    "\n",
    "    def conv_to_seq(self, conversation: List[Dict[str, object]], max_seq_len: int = 0) -> List[int]:\n",
    "        l = [self._word2ind[SOS_TOKEN]]\n",
    "        for conv in conversation:\n",
    "            l.extend(self.sen_to_seq(conv['line'], add_tokens=False))\n",
    "            l.append(self._word2ind[SEP_TOKEN])\n",
    "\n",
    "        if len(l) > 0:\n",
    "            l = l[:max_seq_len]\n",
    "            l[-1] = self._word2ind[EOS_TOKEN]\n",
    "        l += [self._word2ind[PAD_TOKEN]] * (max_seq_len - len(l))\n",
    "        return l\n",
    "        \n",
    "\n",
    "    def gen_mask(self, tokenized_sentence: List[int]) -> List[bool]:\n",
    "        \"\"\"Creates a mask for the given tokenized sentence.\n",
    "\n",
    "        >>> pv = ParsedVocab([('<sos>', 0), ('<eos>', 0), ('hi', 0)])\n",
    "        >>> x = pv.sen_to_seq('<sos> hi <eos> <unk> <unk>', )\n",
    "        >>> pv.gen_mask(x)\n",
    "        [1, 1, 1, 0, 0]\n",
    "        \"\"\"\n",
    "        l = [True] * len(tokenized_sentence)\n",
    "        i = len(tokenized_sentence) - 1\n",
    "        while i >= 0 and tokenized_sentence[i] == self._word2ind[PAD_TOKEN]:\n",
    "            l[i] = False\n",
    "            i -= 1\n",
    "        return l\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Regulard vocabulary for holding the conversations and number of words.\"\"\"\n",
    "\n",
    "    DEFAULT_CONTEXT = 'default'\n",
    "\n",
    "    def __init__(self, conversation_depth: int = 4):\n",
    "        self.words = {}\n",
    "        self._context = Vocab.DEFAULT_CONTEXT\n",
    "        self.conversations = {}\n",
    "        self._held_conversations = {}\n",
    "        self.conversation_depth = conversation_depth\n",
    "        self.longest = 0\n",
    "\n",
    "    def add_word(self, word: str) -> None:\n",
    "        word = word.lower()\n",
    "        if not word in self.words:\n",
    "            self.words[word] = 0\n",
    "        self.words[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence: str) -> None:\n",
    "        sentence = f'{SOS_TOKEN} {sentence} {EOS_TOKEN}'\n",
    "        [self.add_word(s) for s in sentence.split()]\n",
    "\n",
    "    def switch_context(self, new_context: str) -> None:\n",
    "        if self._context in self._held_conversations and len(self._held_conversations[self._context]) > self.conversation_depth:\n",
    "            self.conversations[self._context].append(self._held_conversations[self._context][\n",
    "                -self.conversation_depth:\n",
    "            ])\n",
    "        self._context = new_context\n",
    "\n",
    "    def add_conversation(self, conversation: Dict[str, object]) -> None:\n",
    "        if not self._context in self.conversations:\n",
    "            self.conversations[self._context] = []\n",
    "            self._held_conversations[self._context] = [conversation]\n",
    "            return\n",
    "        self.add_line(conversation)\n",
    "        line = self._held_conversations[self._context][-1]['line'].split()\n",
    "        if len(line) > self.longest:\n",
    "            self.longest = len(line)\n",
    "    \n",
    "    def add_line(self, conversation: Dict[str, object]) -> bool:\n",
    "        if not self._context in self._held_conversations or len(self._held_conversations[self._context]) == 0:\n",
    "            self._held_conversations[self._context] = [conversation]\n",
    "            return True\n",
    "        hc = self._held_conversations[self._context] # Held Conversation\n",
    "        lc = hc[-1] # Last conversation\n",
    "        # Same speaker\n",
    "        if (len(lc['speaker']) > 0 and lc['speaker'] == conversation['speaker']) or \\\n",
    "            (len(lc['speaker']) == 0 and len(conversation['speaker']) == 0 and len(conversation['line']) > 0 and conversation['line'][0].islower()) and \\\n",
    "            conversation['when'] - lc['when'] < 1000 * 60 * 1.5:\n",
    "            hc[-1]['when'] = conversation['when']\n",
    "            hc[-1]['line'] += f\" {conversation['line']}\"\n",
    "            return False\n",
    "        if len(self._held_conversations[self._context]) >= 2:\n",
    "            self.conversations[self._context].append(self._held_conversations[self._context][\n",
    "                -min(self.conversation_depth, len(hc)):\n",
    "            ])\n",
    "        hc.append(conversation)\n",
    "        return True\n",
    "\n",
    "    def parse_vocab(self) -> ParsedVocab:\n",
    "        words = list(self.words.items())\n",
    "        return ParsedVocab(words, self.longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing folder: ditfxx_subs\n",
      "  Opening file: DitFXX (1).ass\n",
      "  Opening file: DitFXX (2).ass\n",
      "  Opening file: DitFXX (3).ass\n",
      "  Opening file: DitFXX (4).ass\n",
      "  Opening file: DitFXX (5).ass\n",
      "  Opening file: DitFXX (6).ass\n",
      "  Opening file: DitFXX (7).ass\n",
      "  Opening file: DitFXX (8).ass\n",
      "  Opening file: DitFXX (9).ass\n",
      "  Opening file: DitFXX (10).ass\n",
      "  Opening file: DitFXX (11).ass\n",
      "  Opening file: DitFXX (12).ass\n",
      "  Opening file: DitFXX (13).ass\n",
      "  Opening file: DitFXX (14).ass\n",
      "  Opening file: DitFXX (15).ass\n",
      "  Opening file: DitFXX (16).ass\n",
      "  Opening file: DitFXX (17).ass\n",
      "  Opening file: DitFXX (18).ass\n",
      "  Opening file: DitFXX (19).ass\n",
      "  Opening file: DitFXX (20).ass\n",
      "  Opening file: DitFXX (21).ass\n",
      "  Opening file: DitFXX (22).ass\n",
      "  Opening file: DitFXX (23).ass\n",
      "Parsing folder: steins_gate_subs\n",
      "  Opening file: Steins;Gate 01.ass\n",
      "  Opening file: Steins;Gate 02.ass\n",
      "  Opening file: Steins;Gate 03.ass\n",
      "  Opening file: Steins;Gate 04.ass\n",
      "  Opening file: Steins;Gate 05.ass\n",
      "  Opening file: Steins;Gate 06.ass\n",
      "  Opening file: Steins;Gate 07.ass\n",
      "  Opening file: Steins;Gate 08.ass\n",
      "  Opening file: Steins;Gate 09.ass\n",
      "  Opening file: Steins;Gate 10.ass\n",
      "  Opening file: Steins;Gate 11.ass\n",
      "  Opening file: Steins;Gate 12.ass\n",
      "  Opening file: Steins;Gate 13.ass\n",
      "  Opening file: Steins;Gate 14.ass\n",
      "  Opening file: Steins;Gate 15.ass\n",
      "  Opening file: Steins;Gate 16.ass\n",
      "  Opening file: Steins;Gate 17.ass\n",
      "  Opening file: Steins;Gate 18.ass\n",
      "  Opening file: Steins;Gate 19.ass\n",
      "  Opening file: Steins;Gate 20.ass\n",
      "  Opening file: Steins;Gate 21.ass\n",
      "  Opening file: Steins;Gate 22.ass\n",
      "  Opening file: Steins;Gate 23.ass\n",
      "  Opening file: Steins;Gate 24.ass\n",
      "  Opening file: Steins;Gate 25.ass\n",
      "Done! Num conversations: 12094, num words: 5485, longest convo: 181\n"
     ]
    }
   ],
   "source": [
    "FOLDERS = ['ditfxx_subs', 'steins_gate_subs']\n",
    "CONVERSATION_DEPTH = 4\n",
    "\n",
    "vocab = Vocab(CONVERSATION_DEPTH)\n",
    "\n",
    "for folder in FOLDERS:\n",
    "    dir = os.listdir(os.path.join('data', folder))\n",
    "    dir.sort(key=match_num)\n",
    "    print(f'Parsing folder: {folder}')\n",
    "    for f in dir:\n",
    "        filepath = os.path.join(os.getcwd(), 'data', folder, f)\n",
    "        if not os.path.isfile(filepath): continue\n",
    "        print(f'  Opening file: {f}')\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as sub_file:\n",
    "            is_event = False\n",
    "            line = True\n",
    "            while not is_event and line:\n",
    "                line = sub_file.readline()\n",
    "                if not line: break\n",
    "                if line.rstrip() == \"[Events]\":\n",
    "                    is_event = True\n",
    "            current_format = False\n",
    "            current_conversation = []\n",
    "            \n",
    "            vocab.switch_context(f)\n",
    "            line = True\n",
    "            # for line in sub_file.readlines():\n",
    "            while line:\n",
    "                try:\n",
    "                    line = sub_file.readline()\n",
    "                except UnicodeDecodeError:\n",
    "                    print('    Error decoding a line, skipped.')\n",
    "                if line.startswith('Format:'):\n",
    "                    line = line[len('Format:'):].strip().split(', ')\n",
    "                    current_format = line\n",
    "                    continue\n",
    "                if current_format == False or not line.startswith('Dialogue:'): continue\n",
    "                line = line[len('Dialogue:'):].strip().split(',')\n",
    "                line[len(current_format)-1] = ','.join(line[len(current_format)-1:])\n",
    "                dialogue = dict(zip(current_format, line))\n",
    "                if not dialogue['Style'] in ['main', 'Default']: continue\n",
    "                # Extract variables\n",
    "                speaker = dialogue['Name']\n",
    "                text = normalize_text(dialogue['Text'])\n",
    "                time = get_time(dialogue['Start'])\n",
    "\n",
    "                # if len(current_conversation) > 0 and time - current_conversation[-1]['when'] > 1000 * 60 * 2:\n",
    "                #     current_conversation = []\n",
    "                # if len(current_conversation) > 0 and ((len(speaker) > 0 and current_conversation[-1]['speaker'] == speaker) or \n",
    "                # (len(speaker) == 0 and len(dialogue['Text']) > 0 and dialogue['Text'][0].islower())):\n",
    "                #     current_conversation[-1]['line'] += f' {text}'\n",
    "                #     current_conversation[-1]['when'] = time\n",
    "                # else:\n",
    "                #     vocab.add_conversation(current_conversation)\n",
    "                #     current_conversation.append({\n",
    "                #         'speaker': speaker,\n",
    "                #         'line': text,\n",
    "                #         'when': time\n",
    "                #     })\n",
    "                # if len(current_conversation) > CONVERSATION_DEPTH:\n",
    "                #     current_conversation = current_conversation[CONVERSATION_DEPTH - len(current_conversation):]\n",
    "                # if len(current_conversation) == 1:\n",
    "                #     continue\n",
    "                vocab.add_conversation({\n",
    "                    'speaker': speaker,\n",
    "                    'line': text,\n",
    "                    'when': time\n",
    "                })\n",
    "                vocab.add_sentence(text)\n",
    "            \n",
    "\n",
    "pv = vocab.parse_vocab()\n",
    "convos = 0\n",
    "for k, c in vocab.conversations.items():\n",
    "    convos += len(c)\n",
    "print(f'Done! Num conversations: {convos}, num words: {len(pv.get_words())}, longest convo: {vocab.longest}')\n",
    "# print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': 'Z2', 'line': 'I wanna take a bath .', 'when': 150},\n",
       " {'speaker': 'Franxx',\n",
       "  'line': 'Not again . Show some self-control .',\n",
       "  'when': 2160}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(vocab.conversations)\n",
    "c = vocab.conversations[x[0]]\n",
    "c[0][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "Defining the actual AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 545, 5485])\n",
      "torch.Size([1, 545])\n"
     ]
    }
   ],
   "source": [
    "# 3 Sentences with 2 delims\n",
    "seq_len = vocab.longest * (vocab.conversation_depth - 1) + 2\n",
    "\n",
    "# Thanks to\n",
    "# https://github.com/lucidrains/performer-pytorch\n",
    "model = PerformerLM(\n",
    "    num_tokens=len(pv.get_words()),\n",
    "    max_seq_len=seq_len,\n",
    "    dim=512,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    causal=False,\n",
    "    nb_features=256,\n",
    "    generalized_attention=False,\n",
    "    kernel_fn=nn.ReLU(),\n",
    "    reversible=True,\n",
    "    ff_chunks=10,\n",
    "    use_scalenorm=False,\n",
    "    use_rezero=True\n",
    ")\n",
    "\n",
    "x = torch.randint(0, len(pv.get_words()), (1, seq_len))\n",
    "mask = torch.ones_like(x).bool()\n",
    "\n",
    "y = model(x, mask=mask)\n",
    "\n",
    "print(y.size())\n",
    "\n",
    "# make_dot(y.mean(), params=dict(model.named_parameters()))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "PRINT_EVERY = 40\n",
    "\n",
    "class ConversationIter:\n",
    "\n",
    "    def __init__(self, vocab: Vocab, parsed_vocab: ParsedVocab, max_seq_len: int):\n",
    "        self._vocab = vocab\n",
    "        self._parsed_vocab = parsed_vocab\n",
    "        self._context = random.choice(list(vocab.conversations))\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self._i = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._context = random.choice(list(self._vocab.conversations))\n",
    "        self._i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._i >= len(self._vocab.conversations[self._context]):\n",
    "            raise StopIteration\n",
    "        x = self._vocab.conversations[self._context][self._i]\n",
    "        input = torch.tensor(self._parsed_vocab.conv_to_seq(x[:-1], self.max_seq_len))\n",
    "        target = torch.tensor(self._parsed_vocab.sen_to_seq(x[-1]['line'], seq_len=self.max_seq_len, add_pad=True))\n",
    "        self._i += 1\n",
    "        return input, target\n",
    "\n",
    "def train(conv_iter: ConversationIter):\n",
    "    model.train()\n",
    "    accrued_loss = 0\n",
    "    start = datetime.now()\n",
    "    for i, (input, target) in enumerate(conv_iter):\n",
    "        input.to(device)\n",
    "        target.to(device)\n",
    "\n",
    "        mask = torch.tensor(pv.gen_mask(input))\n",
    "\n",
    "        input.unsqueeze_(0)\n",
    "        target.unsqueeze_(0)\n",
    "        mask.unsqueeze_(0)\n",
    "\n",
    "#         input = F.one_hot(input, len(pv.get_words()))\n",
    "#         input.transpose_(0, 1)\n",
    "\n",
    "#         print(input.shape, target.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, mask=mask)\n",
    "#         output.transpose_(1, 2)\n",
    "        loss = criterion(output.squeeze(0), target.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accrued_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % PRINT_EVERY == 0:\n",
    "            print(f'  Iter {i+1} (Took {(datetime.now() - start).total_seconds():.3f}s): AverageLoss: {accrued_loss/PRINT_EVERY:.4f}')\n",
    "            accrued_loss = 0\n",
    "            start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch #1 of 40:\n"
     ]
    }
   ],
   "source": [
    "TRAIN_EPOCHS = 40\n",
    "SAVE_EVERY = 5\n",
    "\n",
    "conv_iter = ConversationIter(vocab, pv, seq_len)\n",
    "\n",
    "def save_checkpoint(epoch: int):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, os.path.join(model_dir, f'checkpoints/amadeus-performer-{epoch}.pt'))\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    print(f'Training epoch #{epoch+1} of {TRAIN_EPOCHS}:')\n",
    "    total = datetime.now()\n",
    "    train(conv_iter)\n",
    "    print(f'Epoch {epoch+1} took {(datetime.now()-total).total_seconds():.3f}s\\n\\n')\n",
    "    \n",
    "    if (epoch + 1) % SAVE_EVERY == 0:\n",
    "        print('Saving checkpoint...')\n",
    "        save_checkpoint(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}