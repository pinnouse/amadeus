{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os, sys\n",
    "import re\n",
    "from argparse import ArgumentParser, ArgumentTypeError\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('-o', '--o', dest='output', default='', help='Location of output(s)')\n",
    "parser.add_argument('-g', '--use_cuda', type=str2bool, dest='use_cuda', default=True, help='Use cuda if cuda supported')\n",
    "parser.add_argument('-a', '--artifacts', dest='artifacts', default='', help='Directory to save artifacts such as checkpoints')\n",
    "parser.add_argument('-e', '--epochs', type=int, dest='train_epochs', default=10, help='Number of epochs to train on')\n",
    "parser.add_argument('-p', '--print_every', type=int, dest='print_every', default=100, help='After how many iterations to print a status')\n",
    "parser.add_argument('-t', '--validate_every', type=int, dest='validate_every', default=10, help='After how many epochs to validate loss on test set')\n",
    "parser.add_argument('-T', '--save_every', type=int, dest='save_every', default=0, help='After how many epochs before saving a checkpoint (0 to turn off)')\n",
    "parser.add_argument('-A', '--batch_size', type=int, dest='batch_size', default=1, help='Batch size to train on')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "use_cuda = args.use_cuda and torch.has_cuda\n",
    "\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "\n",
    "model_dir = args.output\n",
    "artifacts_dir = args.artifacts\n",
    "\n",
    "train_epochs = max(args.train_epochs, 1)\n",
    "print_every = max(args.print_every, 1)\n",
    "validate_every = max(args.validate_every, 0)\n",
    "save_every = max(args.save_every, 0)\n",
    "batch_size = max(args.batch_size, 1)"
   ]
  },
  {
   "source": [
    "# Prepare the Data\n",
    "\n",
    "Create vocabulary and load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import Vocab\n",
    "\n",
    "vocab = Vocab(conversation_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parsing folder: ditfxx_subs\n",
      "  Opening file: DitFXX(1).ass\n",
      "  Opening file: DitFXX(2).ass\n",
      "  Opening file: DitFXX(3).ass\n",
      "  Opening file: DitFXX(4).ass\n",
      "  Opening file: DitFXX(5).ass\n",
      "  Opening file: DitFXX(6).ass\n",
      "  Opening file: DitFXX(7).ass\n",
      "  Opening file: DitFXX(8).ass\n",
      "  Opening file: DitFXX(9).ass\n",
      "  Opening file: DitFXX(10).ass\n",
      "  Opening file: DitFXX(11).ass\n",
      "  Opening file: DitFXX(12).ass\n",
      "  Opening file: DitFXX(13).ass\n",
      "  Opening file: DitFXX(14).ass\n",
      "  Opening file: DitFXX(15).ass\n",
      "  Opening file: DitFXX(16).ass\n",
      "  Opening file: DitFXX(17).ass\n",
      "  Opening file: DitFXX(18).ass\n",
      "  Opening file: DitFXX(19).ass\n",
      "  Opening file: DitFXX(20).ass\n",
      "  Opening file: DitFXX(21).ass\n",
      "  Opening file: DitFXX(22).ass\n",
      "  Opening file: DitFXX(23).ass\n",
      "Parsing folder: steins_gate_subs\n",
      "  Opening file: Steins;Gate 01.ass\n",
      "  Opening file: Steins;Gate 02.ass\n",
      "  Opening file: Steins;Gate 03.ass\n",
      "  Opening file: Steins;Gate 04.ass\n",
      "  Opening file: Steins;Gate 05.ass\n",
      "  Opening file: Steins;Gate 06.ass\n",
      "  Opening file: Steins;Gate 07.ass\n",
      "  Opening file: Steins;Gate 08.ass\n",
      "  Opening file: Steins;Gate 09.ass\n",
      "  Opening file: Steins;Gate 10.ass\n",
      "  Opening file: Steins;Gate 11.ass\n",
      "  Opening file: Steins;Gate 12.ass\n",
      "  Opening file: Steins;Gate 13.ass\n",
      "  Opening file: Steins;Gate 14.ass\n",
      "  Opening file: Steins;Gate 15.ass\n",
      "  Opening file: Steins;Gate 16.ass\n",
      "  Opening file: Steins;Gate 17.ass\n",
      "  Opening file: Steins;Gate 18.ass\n",
      "  Opening file: Steins;Gate 19.ass\n",
      "  Opening file: Steins;Gate 20.ass\n",
      "  Opening file: Steins;Gate 21.ass\n",
      "  Opening file: Steins;Gate 22.ass\n",
      "  Opening file: Steins;Gate 23.ass\n",
      "  Opening file: Steins;Gate 24.ass\n",
      "  Opening file: Steins;Gate 25.ass\n",
      "Parsing folder: guilty_crown_subs\n",
      "  Opening file: [Commie] Guilty Crown - 01 [662BB1FD].ass\n",
      "  Opening file: [Commie] Guilty Crown - 02 [6D1930E8].ass\n",
      "  Opening file: [Commie] Guilty Crown - 03 [5EF0B8DB].ass\n",
      "  Opening file: [Commie] Guilty Crown - 04 [D917AC8F].ass\n",
      "  Opening file: [Commie] Guilty Crown - 05 [CEDCE7F8].ass\n",
      "  Opening file: [Commie] Guilty Crown - 06 [88FE8145].ass\n",
      "  Opening file: [Commie] Guilty Crown - 07 [EF5FB5F1].ass\n",
      "  Opening file: [Commie] Guilty Crown - 08 [B171C9BB].ass\n",
      "  Opening file: [Commie] Guilty Crown - 09 [52C0456D].ass\n",
      "  Opening file: [Commie] Guilty Crown - 10 [6094511C].ass\n",
      "  Opening file: [Commie] Guilty Crown - 11 [8C27E959].ass\n",
      "  Opening file: [Commie] Guilty Crown - 12 [243140FE].ass\n",
      "  Opening file: [Commie] Guilty Crown - 13 [19A88BBA].ass\n",
      "  Opening file: [Commie] Guilty Crown - 14 [18AB6B39].ass\n",
      "  Opening file: [Commie] Guilty Crown - 15 [E526E28E].ass\n",
      "  Opening file: [Commie] Guilty Crown - 16 [A9F55A7F].ass\n",
      "  Opening file: [Commie] Guilty Crown - 17 [78831216].ass\n",
      "  Opening file: [Commie] Guilty Crown - 18 [DD3DBE6E].ass\n",
      "  Opening file: [Commie] Guilty Crown - 19 [77E06975].ass\n",
      "  Opening file: [Commie] Guilty Crown - 20 [A98A9A05].ass\n",
      "  Opening file: [Commie] Guilty Crown - 21v2 [012C508C].ass\n",
      "  Opening file: [Commie] Guilty Crown - 22 [1084F246].ass\n",
      "Parsing folder: ngnl_subs\n",
      "  Opening file: [HorribleSubs] No Game No Life - 01 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 02 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 03 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 04 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 05 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 06 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 07 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 08 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 09 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 10 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 11 [720p].ass\n",
      "  Opening file: [HorribleSubs] No Game No Life - 12 [720p].ass\n",
      "Parsing folder: rezero_subs\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 01A [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 01B [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 02 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 03 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 04 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 05 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 06 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 07 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 08 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 09 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 10 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 11 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 12 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 13 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 14 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 15 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 16 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 17 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 18 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 19 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 20 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 21 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 22 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 23 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 24 [720p]_track3_eng.ass\n",
      "  Opening file: [HorribleSubs] Re Zero kara Hajimeru Isekai Seikatsu - 25 [720p]_track3_eng.ass\n",
      "Done! Num conversations: 25221, num words: 9656, longest convo: 298\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FOLDERS = [\n",
    "    'ditfxx_subs', 'steins_gate_subs', 'guilty_crown_subs',\n",
    "    'ngnl_subs', 'rezero_subs', 'promised_neverland_subs', 'your_lie_subs',\n",
    "    'shield_hero_subs', 'fate_ubw_subs'\n",
    "    ]\n",
    "CONVERSATION_DEPTH = 4\n",
    "\n",
    "multiplier = [60, 60 * 60, 24 * 60 * 60]\n",
    "def get_time(timestr: str) -> int:\n",
    "    time = timestr.split(':')\n",
    "    final_time = 0\n",
    "    ms = float(time[-1]) * 1000\n",
    "    final_time += int(ms)\n",
    "    for i in range(len(time)-2):\n",
    "        t = time[-2-i]\n",
    "        final_time += multiplier[i] * int(t)\n",
    "    return final_time\n",
    "\n",
    "normalize_pattern = re.compile(r'(\\{[\\\\\\*][\\w\\(\\)\\\\\\,\\*]*|\\})', re.M)\n",
    "sub_space = re.compile(r'(\\{|\\\\[nN])', re.M)\n",
    "insert_space = re.compile(r'([\\w\\\"])([\\.\\!\\,\\?\\W])')\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = normalize_pattern.sub('', text)\n",
    "    text = sub_space.sub(' ', text)\n",
    "    text = re.sub(r'([\\'\\\"])', r' \\1 ', text)\n",
    "    text = re.sub(r'([\\.\\!\\?\\W])(\\w)', r'\\1 \\2', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return insert_space.sub(r'\\1 \\2', text)\n",
    "\n",
    "number_match = re.compile(r'\\d+')\n",
    "def match_num(text: str) -> int:\n",
    "    x = number_match.findall(text)\n",
    "    return int(x[0] if len(x) > 0 else 0)\n",
    "\n",
    "for folder in FOLDERS:\n",
    "    dir = os.listdir(os.path.join('data', folder))\n",
    "    dir.sort(key=match_num)\n",
    "    print(f'Parsing folder: {folder}')\n",
    "    for f in dir:\n",
    "        filepath = os.path.join(os.getcwd(), 'data', folder, f)\n",
    "        if not os.path.isfile(filepath): continue\n",
    "        print(f'  Opening file: {f}')\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as sub_file:\n",
    "            is_event = False\n",
    "            line = True\n",
    "            while not is_event and line:\n",
    "                line = sub_file.readline()\n",
    "                if not line: break\n",
    "                if line.rstrip() == \"[Events]\":\n",
    "                    is_event = True\n",
    "            current_format = False\n",
    "            current_conversation = []\n",
    "            \n",
    "            vocab.switch_context(f)\n",
    "            line = True\n",
    "            # for line in sub_file.readlines():\n",
    "            while line:\n",
    "                try:\n",
    "                    line = sub_file.readline()\n",
    "                except UnicodeDecodeError:\n",
    "                    print('    Error decoding a line, skipped.')\n",
    "                if line.startswith('Format:'):\n",
    "                    line = line[len('Format:'):].strip().split(', ')\n",
    "                    current_format = line\n",
    "                    continue\n",
    "                if current_format == False or not line.startswith('Dialogue:'): continue\n",
    "                line = line[len('Dialogue:'):].strip().split(',')\n",
    "                line[len(current_format)-1] = ','.join(line[len(current_format)-1:])\n",
    "                dialogue = dict(zip(current_format, line))\n",
    "                if not dialogue['Style'] in ['main', 'Default', 'italics', 'flashback', 'ngnl-main']: continue\n",
    "                # Extract variables\n",
    "                speaker = dialogue['Name']\n",
    "                text = normalize_text(dialogue['Text'])\n",
    "                time = get_time(dialogue['Start'])\n",
    "                style = dialogue['Style']\n",
    "\n",
    "                if len(text.strip()) == 0: continue\n",
    "\n",
    "                vocab.add_conversation({\n",
    "                    'speaker': speaker,\n",
    "                    'line': text,\n",
    "                    'when': time,\n",
    "                    'style': style\n",
    "                })\n",
    "                vocab.add_sentence(text)\n",
    "\n",
    "convos = 0\n",
    "for k, c in vocab.conversations.items():\n",
    "    convos += len(c)\n",
    "\n",
    "print(f'Done! Num conversations: {convos}, num words: {len(vocab.words)}, longest convo: {vocab.longest_tokenized}\\n\\n')"
   ]
  },
  {
   "source": [
    "# Create the Model\n",
    "\n",
    "Using preset hyperparameters from Amadeus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1192, 1192)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from amadeus_model import Amadeus\n",
    "\n",
    "model = Amadeus(num_tokens=vocab.tokenizer.get_vocab_size(), \\\n",
    "    enc_seq_len=2048, dec_seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "in_seq = torch.randint(0, vocab.tokenizer.get_vocab_size(), (1, model.in_seq_len))\n",
    "out_seq = torch.randint(0, vocab.tokenizer.get_vocab_size(), (1, model.out_seq_len))\n",
    "mask = torch.ones(1, model.in_seq_len).bool()\n",
    "\n",
    "y = model(in_seq, out_seq, mask=mask)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "source": [
    "# Train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split train/test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from vocab import ConversationIter\n",
    "\n",
    "conversations = list(vocab.conversations.values())\n",
    "\n",
    "train_set, test_set = train_test_split(conversations, test_size=0.2)\n",
    "\n",
    "train_set = ConversationIter(train_set, in_seq_len=model.in_seq_len, \\\n",
    "    out_seq_len=model.out_seq_len, tokenizer=vocab.tokenizer, batch_size=batch_size)\n",
    "test_set = ConversationIter(test_set, in_seq_len=model.in_seq_len, \\\n",
    "    out_seq_len=model.out_seq_len, tokenizer=vocab.tokenizer, batch_size=batch_size)"
   ]
  },
  {
   "source": [
    "## Train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adafactor import Adafactor\n",
    "\n",
    "has_gradient = False\n",
    "try:\n",
    "    from gradient_statsd import Client\n",
    "    has_gradient = True\n",
    "    client = Client()\n",
    "except ImportError:\n",
    "    print('gradient_statsd package is not installed, not using gradient metrics.')\n",
    "\n",
    "optimizer = Adafactor(model.parameters())\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "def format_time(dt: datetime) -> str:\n",
    "    return format(dt, '%Y-%m-%d-%H.%M.%S')\n",
    "\n",
    "def train(conv_iter: ConversationIter):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    accrued_loss = 0\n",
    "    start = datetime.now()\n",
    "    for i, (inputs, targets) in enumerate(conv_iter):\n",
    "        mask = torch.tensor([inp.attention_mask for inp in inputs]).bool()\n",
    "\n",
    "        inputs = torch.tensor([inp.ids for inp in inputs])\n",
    "        targets = torch.tensor([tar.ids for tar in targets])\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            mask = mask.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(inputs, targets, mask=mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accrued_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        counter += 1\n",
    "        \n",
    "        if (i + 1) % print_every == 0:\n",
    "            print(f'  Iter {i+1} (Took {(datetime.now() - start).total_seconds():.3f}s): AverageLoss: {accrued_loss/print_every:.4f}')\n",
    "            total_loss += accrued_loss\n",
    "            accrued_loss = 0\n",
    "            start = datetime.now()\n",
    "    return total_loss / max(counter, 1)\n",
    "\n",
    "def validate(conv_iter: ConversationIter):\n",
    "    model.eval(False)\n",
    "    with torch.no_grad():\n",
    "        inp, tar = conv_iter.random_sample(pad_in=True)\n",
    "        mask = torch.tensor([i.attention_mask for i in inp]).bool()\n",
    "        inputs = torch.tensor([i.ids for i in inp])\n",
    "        targets = torch.tensor([t.ids for t in tar])\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            mask = mask.cuda()\n",
    "        \n",
    "        loss = model(inputs, targets, mask=mask)\n",
    "        print(f'Validation loss: {loss.item()}')\n",
    "    return loss.item()\n",
    "\n",
    "def save_checkpoint(epoch: int):\n",
    "    Path(os.path.join(artifacts_dir, 'checkpoints')).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, os.path.join(artifacts_dir, 'checkpoints', f'amadeus-performer-{format_time(start_time)}-{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting train on device: cuda\n",
      "\n",
      "Training epoch #1 of 20:\n",
      "========================\n",
      "  Iter 100 (Took 88.722s): AverageLoss: 6.7320\n",
      "  Iter 200 (Took 88.217s): AverageLoss: 5.0536\n",
      "  Iter 300 (Took 88.406s): AverageLoss: 5.0165\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'total' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-af8b31b04e59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch+1} took {(datetime.now()-total).total_seconds():.3f}s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalidate_every\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Starting train on device: {device}')\n",
    "print(f'Training on {train_epochs} epochs with batch size of {batch_size}')\n",
    "print(f'Validating every {validate_every} and saving every {save_every}\\n')\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    prompt = f'Training epoch #{epoch+1} of {train_epochs}:'\n",
    "    print(f'{prompt}\\n{\"=\" * len(prompt)}')\n",
    "\n",
    "    total = datetime.now()\n",
    "\n",
    "    total_loss = train(train_set)\n",
    "\n",
    "    if has_gradient:\n",
    "        client.increment('EPOCHS', 1)\n",
    "        client.gauge('LOSS_PER_EPOCH', total_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1} took {(datetime.now()-total).total_seconds():.3f}s')\n",
    "\n",
    "    if validate_every > 0 and (epoch + 1) % validate_every == 0:\n",
    "        validate_loss = validate(test_set)\n",
    "        if has_gradient:\n",
    "            client.gauge('VALIDATE_LOSS', validate_loss)\n",
    "\n",
    "    if save_every > 0 and (epoch + 1) % save_every == 0:\n",
    "        save_checkpoint(epoch + 1)\n",
    "\n",
    "    print('\\n\\n')\n",
    "\n",
    "Path(os.path.join(model_dir, 'models')).mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(model_dir, 'models', f'amadeus-performer-{format_time(start_time)}.pt'))\n",
    "print('Finished training and saved model in models directory.')"
   ]
  }
 ]
}